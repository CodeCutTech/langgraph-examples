# Prompting Isnâ€™t Enough: Why LLM Workflows Need Graphs, Not Chains

Ever tried building a support assistant with just prompts or chainsâ€”only to watch it hallucinate, forget what it was doing, or get stuck in loops?

Prompt chaining (like LangChain) helps a bit. But real-world workflows donâ€™t move in straight lines. They loop, branch, retry, and rely on shared memory. Prompt chains are recipes; real tasks are flowcharts.

Thatâ€™s where **LangGraph** comes in.

It treats your logic as a **graph**: each step is a node, transitions are edges, and a shared state evolves as the system runs. Itâ€™s like a state machine, but for LLMsâ€”and itâ€™s shockingly useful.

Letâ€™s build a small, concrete example to show you how.

---

## ğŸ” Problem: Analyze a Customerâ€™s History

Suppose we want an assistant that reviews a customerâ€™s history:

* Fetch their profile
* Read feedback theyâ€™ve left
* Check support tickets
* Summarize it all

This isnâ€™t a strict linear pipelineâ€”it might skip steps, retry them, or use early exits. Instead of forcing it into a rigid sequence, weâ€™ll build a **graph** of steps that share evolving context.

---

## ğŸ› ï¸ Setup

To follow along, install the basics:

```bash
pip install langgraph openai pydantic
```

Weâ€™ll simulate tools that return structured data using Pydantic. Think of these as stand-ins for real API calls or database queries.

---

## ğŸ“¦ Types and Tools

First, define the data we'll be working with:

```python
from pydantic import BaseModel

class Profile(BaseModel):
    id: str
    name: str
    segment: str

class Feedback(BaseModel):
    customer_id: str
    comments: list[str]

class Ticket(BaseModel):
    id: str
    status: str
    issue: str
```

Now, our dummy tools. Each one returns part of the customerâ€™s history:

```python
def get_profile(state):
    return {"profile": Profile(id="123", name="Jane", segment="premium")}

def get_feedback(state):
    return {"feedback": Feedback(customer_id="123", comments=["Great product!", "Support was slow."])}

def get_tickets(state):
    return {"tickets": [Ticket(id="t1", status="open", issue="Login failed")]}
```

These operate on a shared `state`, adding structured results at each step.

---

## ğŸ§  Shared State Model

To clarify what our graph is passing around, define a simple state schema:

```python
class State(BaseModel):
    profile: Profile | None = None
    feedback: Feedback | None = None
    tickets: list[Ticket] | None = None
```

You could enforce this in LangGraph, but even using plain dicts, it's helpful to know what your workflow state is supposed to look like.

---

## ğŸ”— Define the Graph

Now letâ€™s wire up the workflow using LangGraph:

```python
from langgraph.graph import StateGraph

builder = StateGraph(dict)  # dict or State, depending on how strict you want to be

builder.add_node("profile", get_profile)
builder.add_node("feedback", get_feedback)
builder.add_node("tickets", get_tickets)

builder.set_entry_point("profile")
builder.add_edge("profile", "feedback")
builder.add_edge("feedback", "tickets")

graph = builder.compile()
```

Each node gets access to everything upstream. Youâ€™re not just passing output to inputâ€”youâ€™re growing a shared, persistent state.

---

## â–¶ï¸ Run the Workflow

You can stream the graphâ€™s execution step-by-step:

```python
if __name__ == "__main__":
    for step in graph.stream({}, {"recursion_limit": 5}):
        print("--- step ---")
        print(step)
```

Sample output:

```
--- step ---
{'profile': Profile(id='123', name='Jane', segment='premium')}
--- step ---
{'profile': ..., 'feedback': Feedback(...)}
--- step ---
{'profile': ..., 'feedback': ..., 'tickets': [...]}
```

At each step, you get a full snapshot of the accumulated stateâ€”perfect for debugging or introspecting complex flows.

---

## ğŸ” Visualize the Flow

LangGraph can even draw your workflow.

In a notebook, try:

```python
from IPython.display import Image, display

try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception:
    print("Optional: install graphviz and Mermaid dependencies for rendering.")
```

Output (in Mermaid syntax):

```
graph LR
  profile --> feedback --> tickets
```

A clear trail. Linear nowâ€”but extensible.

---

## ğŸ§  Takeaways

* **LangGraph manages memory** across stepsâ€”no more juggling prompt injections
* Workflows become **transparent**: inspect state at any point
* **Modular logic**: each tool just mutates shared state
* You can easily **extend** with branches, loops, or conditionals

---

## ğŸš€ Why This Matters

Prompts are powerful. But orchestration? Thatâ€™s where most LLM projects break down.

You need something between â€œjust glue some promptsâ€ and â€œbuild a full backend.â€ Graphs give you that middle ground: composable, stateful, and debuggable logic for real-world assistants.

LangGraph is that tool. It's not magicâ€”but it's exactly enough structure to stop duct-taping your AI workflows together.
